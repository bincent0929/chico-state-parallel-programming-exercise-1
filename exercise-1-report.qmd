---
title: "Parallel Exercise 1 Report"
format:
  html:
    code-fold: true
jupyter: python3
---

## Part 1

### a):

When running `hello_omp.c`, the program outputs a "Hello" message from each thread. It does appear that the output is accomplished out of the order of the rank of the thread.

This was my output:

``` bash
usage: hello_omp <number threads>
Main program thread will now create all threads requested ...
Hello from OMP thread 0 of 4
Hello from OMP thread 3 of 4
Hello from OMP thread 1 of 4
Hello from OMP thread 2 of 4
All threads now done, main program proceeding to exi
```

With it going from 0, 3, 1, 2.

After reading the documentation, I went ahead and actually updated the function itself (I later realized I could've just put a look around the function instead).

I moved the `omp` `pragma` into the `Hello_threads` function. I changed the function to include a parameter for the `thread_count` so that each thread didn't have to pull the value. Then I created a `for` loop around the `printf`.

It took me a while to learn, but I realized that I could create a "block" that `omp` could parallelize. I could do this by putting `{}` around whatever I wanted parallelized.

I decided to move the `my_rank` into the block so that each thread got its private rank. And, in addition to that, the `for` loop was also included with each thread having its own `i` value so that their iteration position wasn't changed by another thread.

This made it so that each thread would print out the same statement four times, individually. Which results in the program printing out 16 results. \### b): After compiling `dct2.c` and running it, it runs for a little while and then terminates with no output.

When I used the `bash` `time` function, I timed the function to take, this was the output:

``` bash
real    0m11.704s
user    0m11.700s
sys     0m0.001s
```

```{python}
dct2_runtime = 11.704
```

In reading the comments in `dct2.c` it seems that 3 frames are being processed based on this segment of code and comments:

``` c
// E.g. Since video is most often 30 Hz, or 30 frames/sec, 30 iterations is therefor like 1 second of video.
//      Adjust the iterations as is reasonable for your system!
//
#define MAX_ITERATIONS (3)
```

```{python}
dct2_frames = 3
```

With this information we have `{python} dct2_frames` frames and `{python} dct2_runtime` seconds of the program running.

```{python}
dct2_time_per_frame = dct2_runtime / dct2_frames
```

With the result being around `{python} f'{dct2_time_per_frame:.2f}'` s/frame.

### c):

After compiling `ompdct2` and running with the `time` bash function, the results were:

```{python}
ompdct2_total_time = 4.253
ompdct2_frames = 3
ompdct2_time_per_frame = ompdct2_total_time / ompdct2_frames
```

With the result being around `{python} f'{ompdct2_time_per_frame:.2f}'` s/frame

Now I'll do a calculation for how much the speedup was:

``` {python}
omp_dct_speedup = dct2_time_per_frame / ompdct2_time_per_frame
```

The speed-up with `omp` is `{python} f'{omp_dct_speedup:.2f}'`x over not using `omp`.

### d):
``` {python}
import matplotlib.pyplot as plt

# thread : runtime
runtimes = {1: 11.677,
            2:7.859,
            3:4.057,
            4:4.031}

x = list(runtimes.keys())
y = list(runtimes.values())

plt.plot(x, y, 'o-')
plt.xlabel('Number of Threads')
plt.ylabel('Runtime (seconds)')
plt.title('Runtime vs Number of Threads')
plt.grid(True)

plt.xticks(list(runtimes.keys()))

plt.show()
```

### e):

The parallel portion of the code is everything in the `for` loop that comes just after the `pragma` in the `main` function within `ompdct2.c`. Everything else in the program is sequential.

This `for` loop composes a large portion of the program. It has other `for` loops within it and the `dct` and `idct` functions. By Ahmdal's law, this means that, theoretically, because most of the time of the code is spend on the parallelized portion, the program, overall, will completed much quicker.

Determining exactly how much time is spent on the parallelized portion would basically result in it being equal to the time it takes to complete the whole of the program.

## Part 2

### a):

#### Modifications

##### sharpen_grid.c

For the `sharpen_grid.c` program, I didn't make any changes to it and used the default `900` iterations for sharpening the image and got `43.815142 FPS`.

The output looked a little weird to me when I opened it with an image viewer. So I tried to lower the iterations to `20` iterations, but it still came out looking weird. From `20` iterations I got `44.310001 FPS`. 

##### shared.c I got this when I ran the non-OpenMP `sharpen.c`:

``` bash
start test at 0.022464
stop test at 11.696272 for 90 frames
```

``` {python}
sharpenc_frames = 90
sharpenc_start = 0.022464
sharpenc_stop = 11.696272
sharpenc_runtime = sharpenc_stop - sharpenc_start
sharpenc_fps = sharpenc_frames / sharpenc_runtime
```

Doing the math it comes out to: `{python} f'{sharpenc_fps:.2f}'` FPS.

I created a copy of the `sharpen.c` in `ompsharpen.c` and place a `#pragma` just before the iterations loop and this was the outcome for the test:
``` bash
start test at 0.019796
stop test at 4.547733 for 90 frames
```

``` {python}
# this was at 4 threads for ompsharpen
ompsharpenc_frames = 90
ompsharpenc_start = 0.019796
ompsharpenc_stop = 4.547733
ompsharpenc_runtime = ompsharpenc_stop - ompsharpenc_start
ompsharpenc_fps = ompsharpenc_frames / ompsharpenc_runtime
```

The outcome was `{python} f'{ompsharpenc_fps:.2f}'` FPS.

#### PSF Convolution Description

::: {.callout-warning}
Need to read the book and write a description
:::

### b):

``` {python}
sharpen_grid_runtime = 2.031316
```

The runtime for $90$ iterations for the non-paralleized `sharpen.c` was `{python} f'{sharpenc_runtime:.2f}'` seconds and for the parallelized `sharpen_grid.c` implementation at $90$ iterations, its runtime was `{python} f'{sharpen_grid_runtime:.2f}'` seconds.

``` {python}
sharpen_parallel_speed_up =  sharpenc_runtime / sharpen_grid_runtime
```

The `sharpen_grid.c` was much faster than `sharpen.c`. With a result of the parallelize grid processing being `{python} f'{sharpen_parallel_speed_up:.2f}'`x faster than sequential processing.

### c): and d):

Now let's go ahead and compare all the runtimes of the implementations.

We have a runtimes of `{python} f'{sharpenc_runtime:.2f}'` seconds for the non-parallelization `sharpen.c` program. We have `{python} f'{ompsharpenc_runtime:.2f}'` seconds for the OpenMP `ompsharpen.c` program at 4 threads, and `{python} f'{sharpen_grid_runtime:.2f}'` seconds for the `sharpen_grid.c` PThreads implementation of the program.

The fastest is the PThreads implementation, then the OpenMP implementation, and then, naturally, the slowest is the non-parallelized implementation.

A thing that stands out is the fact that PThreads is noticeably faster than the OpenMP implementation.

``` {python}
pthreads_vs_omp_speedup = ompsharpenc_runtime / sharpen_grid_runtime
```

With the resulting speed being `{python} f'{pthreads_vs_omp_speedup:.2f}'`x for PThreads vs. OpenMP.

``` {python}
ompsharpenc_runtime_moret_4 = 2.926082
pthreads_vs_omp_moret_4_speedup = ompsharpenc_runtime_moret_4 / sharpen_grid_runtime
```

Even when I change the amount of threads that the OpenMP program can use, it seems to cap out at around a 2.90 seconds runtime. Which is still comes out with the PThreads implementation resulting in a `{python} f'{pthreads_vs_omp_moret_4_speedup:.2f}'`x speedup vs. OpenMP.

::: {.callout-warning}
Need to compare the results to Amdahl's law and lienar speed-up on a graph for the 4x3 thread gridding I test it on. With "S" being assumed to being the # of cores of the system.
:::